{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_dataset(dir='caltech-101/101_ObjectCategories/', image_size=(96,96), batch_size=32):\n",
    "    \"\"\"Loads the Caltech-101 Dataset downloaded in directory `dir`.\n",
    "\n",
    "    Args:\n",
    "        dir (str): The directory where the caltech-101 dataset is stored. Defaults to 'caltech-101/101_ObjectCategories/'.\n",
    "        image_size (int x int): The width and height of images to rescale directory dataset to.\n",
    "        batch_size (int): The size of batches. Change depending on computational performance.\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.BatchDataset: A Tensorflow BatchDataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        caltech_dataset = image_dataset_from_directory(directory='caltech-101/101_ObjectCategories/',\n",
    "                                                labels='inferred',\n",
    "                                                label_mode='categorical',\n",
    "                                                batch_size=batch_size,\n",
    "                                                image_size=image_size)\n",
    "    except:\n",
    "        \"Please ensure you have downloaded the Caltech-101 Dataset from https://data.caltech.edu/records/mzrjq-6wc02 \\n\"\n",
    "        f\"and placed it into {dir}\"\n",
    "    return caltech_dataset\n",
    "\n",
    "\n",
    "def create_HOG_descriptors(dataset)->np.ndarray:\n",
    "    \"\"\"Creates HOG descriptors of an input dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.BatchDataset): A batched Tensorflow dataset.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A nxp sized NumPy array were each row denotes a HOG descriptor of one input image.\n",
    "    \"\"\"\n",
    "    winSize, blockSize, blockStride, cellSize = (64,64), (16,16), (8,8), (8,8)\n",
    "    nbins, derivAperture, winSigma, histogramNormType = 9, 1, 4.0, 0\n",
    "    L2HysThreshold, gammaCorrection, nlevels = 2e-1, 0, 64\n",
    "\n",
    "    winStride, padding, locations = (8,8), (8,8), ((10,20),)\n",
    "\n",
    "    hogs = []\n",
    "    hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins,derivAperture,winSigma,\n",
    "                            histogramNormType,L2HysThreshold,gammaCorrection,nlevels)\n",
    "    for x,_ in dataset:\n",
    "        for x_item in x:\n",
    "            item = np.round(x_item.numpy()).astype(np.uint8)\n",
    "            h = hog.compute(item,winStride,padding,locations)\n",
    "            hogs.append(h.reshape(1,-1))\n",
    "    return np.concatenate(hogs, axis=0)\n",
    "\n",
    "\n",
    "def normalise_HOGs(hogs:np.ndarray)->np.ndarray:\n",
    "    \"\"\"Normalise HOG descriptor features with mean = 0, sd = 1.\n",
    "\n",
    "    Args:\n",
    "        hogs (np.ndarray): A nxp sized NumPy array of HOG descriptors.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A nxp sized NumPy array of HOG descriptors with normalised features.\n",
    "    \"\"\"\n",
    "    for c in range(hogs.shape[1]):\n",
    "        hogs[:,c] -= hogs[:,c].mean()\n",
    "        if hogs[:,c].std() != 0:\n",
    "            hogs[:,c] /= hogs[:,c].std()\n",
    "    return hogs\n",
    "\n",
    "\n",
    "def reduce_with_PCA(data:np.ndarray, n_components:int=50)->np.ndarray:\n",
    "    \"\"\"Do PCA on data, keeping the top `n_components`.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): A nxp sized NumPy array of data to do PCA with.\n",
    "                           Features are assumed to be normalised.\n",
    "                           The Euclidean distance metric is used.\n",
    "        n_components (int, optional): The number of most varied features to keep. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A nxn_components sized NumPy array of PCA reduced data.\n",
    "    \"\"\"\n",
    "    PCA_reduction = PCA(n_components=n_components)\n",
    "    return PCA_reduction.fit_transform(data)\n",
    "\n",
    "\n",
    "def KMeans_Elbow(data:np.ndarray, cluster_range=(2,100))->None:\n",
    "    \"\"\"Apply KMeans to data `cluster_range[1]-cluster_range[0]` times. \n",
    "       Compute and plot cluster distortions for each cluster count in `cluster_range`.\n",
    "       Predict optimal cluster count based on the elbow method.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): A nxp sized NumPy array of data to apply KMeans clustering to.\n",
    "        cluster_range (tuple, optional): The range of clusters of consider for KMeans and analysis. Defaults to (2,100).\n",
    "    \"\"\"\n",
    "    model = KMeans()\n",
    "    visualizer = KElbowVisualizer(model, k=cluster_range)\n",
    "    visualizer.fit(data)\n",
    "    visualizer.show()\n",
    "\n",
    "\n",
    "def KMeans_clustering(data:np.ndarray, n_clusters:int=45)->np.ndarray:\n",
    "    \"\"\"Apply KMeans clustering to `data` using `n_clusters` clusters.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): A nxp sized NumPy array.\n",
    "        n_clusters (int, optional): The number of clusters to use in the KMeans algorithm. Defaults to 45.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A (data.shape[0],) sized NumPy array denoting the cluster number each datapoint is assigned to.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters).fit(data)\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4700 files belonging to 45 classes.\n"
     ]
    }
   ],
   "source": [
    "caltech_dataset = load_dataset()\n",
    "hog_descriptors = create_HOG_descriptors(caltech_dataset)\n",
    "hog_descriptors_normalised = normalise_HOGs(hog_descriptors)\n",
    "hog_descriptors_PCA = reduce_with_PCA(hog_descriptors_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_Elbow(hog_descriptors_PCA, cluster_range=(2,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37,  1,  1, ..., 29, 27,  1], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KMeans_clustering(hog_descriptors_PCA, n_clusters=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('interview': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb06e563685776455ca3ec8b634de9da7b7fd8355d4bf3bfd1fa4bb0cc316974"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
